{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "suburban.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNglIfIl34/dJ/oHZjpzyFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cactode/suburban/blob/main/suburban.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfmvZsAepBCX"
      },
      "source": [
        "# Suburban Dictionary Fine Tuning Scripts\n",
        "This notebook goes through the process of filtering / cleaning an Urban Dictionary definition corpus and using it to fine-tune a Tensorflow GPT-2 model from Huggingface. It tries to take advantage of Google Colab TPUs, so make sure you have the TPU runtime enabled.\n",
        "\n",
        "See the final result at [cactode.club/suburban](https://cactode.club/suburban)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdsClgfyoog2"
      },
      "source": [
        "If you're starting out with [this raw dataset from Kaggle](https://www.kaggle.com/therohk/urban-dictionary-words-dataset), you'll need to clean it up first with this script.\n",
        "\n",
        "Code for cleaning original dataset and loading it into Arrow format.\n",
        "```python\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "tokenizer = re.compile(r'(^\\d+,)(.+)(,-?\\d+,-?\\d+,\\w*,\\\"[^\\\"]+\\\"$)')\n",
        "inp_lines = None\n",
        "with open('urbandict-word-defs.csv', 'r') as inp:\n",
        "    with open('urbandict-word-defs-fixed.csv', \"w\") as out:\n",
        "        inp_iter = iter(inp)\n",
        "        # skip header\n",
        "        out.write(next(inp_iter) + '\\n')\n",
        "        for inp_line in inp_iter:\n",
        "            match = tokenizer.match(inp_line)\n",
        "            if not match:\n",
        "                print(\"No match...\")\n",
        "                print(inp_line)\n",
        "                continue\n",
        "            elif r'\"' in match[2]:\n",
        "                raise InputException(\"Detected quote, \" + inp_line)\n",
        "            out_line = match.expand(r'\\1\"\\2\"\\3') + '\\n'\n",
        "            out.write(out_line)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW88SFoxY7MP"
      },
      "source": [
        "## Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fXglgTNVqu4"
      },
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!sudo apt-get install git-lfs\n",
        "!pip install datasets transformers symspellpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hKpo9CwZAbn"
      },
      "source": [
        "## Imports and Data Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMX9Rem9BU_-"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "import re\n",
        "import pkg_resources\n",
        "from unicodedata import normalize\n",
        "from datasets import Dataset\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, pipeline\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "from base64 import b64encode, b64decode\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "# utility to print out things with pretty HTML formatting\n",
        "def niceprint(*args):\n",
        "    display(HTML(f\"<p>{' '.join([str(i) for i in args])}\"))\n",
        "\n",
        "# prevents pandas from cutting off printed outputs\n",
        "pd.options.display.max_colwidth = 6000\n",
        "pd.options.display.max_rows = 400\n",
        "\n",
        "# fast spell correct\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "dictionary_path = pkg_resources.resource_filename(\n",
        "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
        "_ = sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "\n",
        "# our source material is inherently racist, but it's worth trying to cut that down by filtering out\n",
        "#  definitions with racist words. racist words are base64 encoded, view at own risk\n",
        "badwords_b64 = b'YXBlIGFyeWFuIGJlYW5lciBiaW1ibyBib290bGlwIGNoaW5rIGNvb2xpZSBjb29uIGNyYWNrZXIgZHlrZSBmYWcgZmFnb3QgZmFnZ290IGdyaW5nbyBob25reSBpbmp1biBpc2xhbSBqYXAgamV3IGpld2lzaCBqaWdhYm9vIGt5a2UgbGVzYm8gbXVzbGltIG5pZ2xldCBuaWdnZXIgbmlnZ2EgcmV0YXJkIHJldGFyZGVkIHNwaWMgd2V0YmFjayB3aG9yZQ=='\n",
        "bad_words = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
        "for word in b64decode(badwords_b64).decode('ascii').split(\" \"):\n",
        "    bad_words.create_dictionary_entry(word, 1)\n",
        "\n",
        "# assuming our data is in our drive under urbandict_word_defs_fixed.csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PREFIX = os.path.join('/content','drive', 'MyDrive', 'Colab Notebooks')\n",
        "\n",
        "# tpu magic\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fVxfusxZNoV"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTHzVxrzXyzp"
      },
      "source": [
        "# values used to filter out bad definitions\n",
        "MIN_UPVOTE = 10\n",
        "MIN_UPDOWN_RATIO = 0.5\n",
        "MIN_WORD_CHARS = 3\n",
        "MIN_DEFINITION_CHARS = 10\n",
        "\n",
        "# influences maximum output length\n",
        "MAX_TOKENS = 128\n",
        "\n",
        "# this is about good for TPUs\n",
        "BATCH_SIZE = 24 * tpu_strategy.num_replicas_in_sync"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOFlRUPTiyzE"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZcJGEmzDzwk"
      },
      "source": [
        "# regex hell. it's pretty far from a science, but it works lol\n",
        "NEWLINES = re.compile(r\" ;; \")\n",
        "FIRST_DEFS = re.compile(r\"^[1a][\\.\\)](?!\\d)\\s?|^1\\s\")\n",
        "MULTIPLE_DEFS = re.compile(r\"\\s[1-6a-g][\\.\\)](?!\\d)\\s?|\\n\\d\\s\")\n",
        "TYPES = re.compile(r\"^(acronym|pronoun|p\\.\\s?noun|noun|adjective|verb|adv\\.|adj\\.|v\\.|n\\.|p\\.)[:\\s]+|^\\([\\w\\s\\.]+\\)[:\\s]*\")\n",
        "REMOVE_NEWLINES = re.compile(r\"(?<=[.,;:\\-]) *\\n+\")\n",
        "REMOVE_NEWLINES_NOPERIOD = re.compile(r\"(?<=\\w) *\\n+\")\n",
        "GARBAGE = GARBAGE = re.compile(r\"[\\[\\]\\{\\}\\\\\\|_\\^\\*`~<>]\")\n",
        "REAL_WORD = re.compile(r\"(?<=[\\s,;:])([a-z]{3}[a-z]*)(?=$|\\s|\\.\\s|[,;:\\?\\!])\")\n",
        "EXCESS_SPACE = re.compile(r\"(\\s)\\s+\")\n",
        "CORRECT_PUNCTUATION = re.compile(r\"([a-z]+)\\s+([\\.,;:])\")\n",
        "ADD_PERIOD = re.compile(r\"([^\\.\\,\\?\\!\\s])$\")\n",
        "\n",
        "# converts everything to lowercase ascii to reduce token burden\n",
        "def normalize_text(text):\n",
        "    return normalize('NFKD', text.strip().lower()).encode('ascii', 'ignore').decode()\n",
        "\n",
        "# helper function that tries to split multiple definitions for one word into unique rows\n",
        "def split_into_lists(definition):\n",
        "    out = normalize_text(definition)\n",
        "    out = NEWLINES.sub(\"\\n\", out)\n",
        "    out = FIRST_DEFS.sub(r\"\", out)\n",
        "    out = MULTIPLE_DEFS.sub(r\"<|split|>\", out).split(\"<|split|>\")\n",
        "    return out\n",
        "\n",
        "# read in our csv and apply data converters defined above. also explode along definition\n",
        "df = pd.read_csv(\n",
        "    os.path.join(PREFIX, 'input_data', 'rawdatasets', 'urbandict-word-defs-fixed.csv'),\n",
        "    usecols=['word', 'up_votes', 'down_votes', 'definition'],\n",
        "    dtype={\n",
        "        'up_votes': 'Int32',\n",
        "        'down_votes': 'Int32',\n",
        "    },\n",
        "    converters = {\n",
        "        'word': normalize_text,\n",
        "        'definition': split_into_lists \n",
        "    },\n",
        "    nrows=1000\n",
        ").dropna().explode('definition', ignore_index=True)\n",
        "\n",
        "# make arrow dataset for next operations\n",
        "ds = Dataset.from_pandas(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc3mCWAWX6P9"
      },
      "source": [
        "# use our regex pile to clean up the word\n",
        "def process_word(word):\n",
        "    out = GARBAGE.sub(\" \", word) # remove pointless fancy characters\n",
        "    out = EXCESS_SPACE.sub(r\"\\1\", out) # remove any more than two spaces\n",
        "    return out\n",
        "\n",
        "# use our regex pile to clean up the definitions\n",
        "def process_definition(definition):\n",
        "    out = definition.strip()\n",
        "    out = TYPES.sub(\"\", out) # remove annoying extra word type specifiers\n",
        "    out = GARBAGE.sub(\" \", out) # remove pointless fancy characters\n",
        "    out = REMOVE_NEWLINES.sub(\" \", out) # remove newlines and replace them with spaces\n",
        "    out = REMOVE_NEWLINES_NOPERIOD.sub(\". \", out) # remove newlines with no periods and add periods, too\n",
        "    out = EXCESS_SPACE.sub(r\"\\1\", out) # remove any more than two spaces\n",
        "    out = CORRECT_PUNCTUATION.sub(r\"\\1\\2\", out) # get rid of spaces before punctuation.\n",
        "    out = ADD_PERIOD.sub(r\"\\1.\", out) # add period to end of sentence.\n",
        "    out = REAL_WORD.sub(\n",
        "        lambda match: sym_spell.lookup(\n",
        "            match[0], \n",
        "            Verbosity.CLOSEST, \n",
        "            max_edit_distance=2,\n",
        "            transfer_casing=True,\n",
        "            include_unknown=True\n",
        "        )[0].term, out\n",
        "    ) # aggressive spellchecking\n",
        "    out = out.strip() # for the road\n",
        "    return out\n",
        "\n",
        "# cleans up words and definitions\n",
        "def process_words_and_definitions(row):\n",
        "    row['word'] = process_word(row['word'])\n",
        "    row['definition'] = process_definition(row['definition'])\n",
        "    return row\n",
        "\n",
        "# assembles both words and definitions into a prompt\n",
        "def assemble_input_text(row):\n",
        "    row['input_text'] = f\"define {row['word']}: {row['definition']}\"\n",
        "    return row\n",
        "    \n",
        "# remove things that have bad upvote records\n",
        "ds = ds.filter(\n",
        "    lambda x: x['up_votes'] > MIN_UPVOTE and \n",
        "    x['down_votes'] and \n",
        "    x['up_votes'] / x['down_votes'] > MIN_UPDOWN_RATIO\n",
        ")\n",
        "\n",
        "# actually map our processing function over everything\n",
        "ds = ds.map(process_words_and_definitions)\n",
        "\n",
        "# remove things that are too short or have racist language\n",
        "ds = ds.filter(\n",
        "    lambda x: len(x['word']) > MIN_WORD_CHARS and\n",
        "    len(x['definition']) > MIN_DEFINITION_CHARS and\n",
        "    not any([bad_words.lookup(word, Verbosity.CLOSEST) for word in REAL_WORD.findall(x['definition'])])\n",
        ")\n",
        "\n",
        "# turn into a single prompt\n",
        "ds = ds.map(assemble_input_text)\n",
        "\n",
        "# eliminate useless columns\n",
        "ds = ds.remove_columns(['word', 'up_votes', 'down_votes', 'definition'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqDhuXa3i5pM"
      },
      "source": [
        "## Data Tokenization and Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvSvbclLYBxm"
      },
      "source": [
        "# prepare tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\n",
        "    'gpt2',\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# define tokenizing function\n",
        "def tokenize(in_ds):\n",
        "    # initial tokenization\n",
        "    token_data = tokenizer(\n",
        "        in_ds['input_text'],\n",
        "        max_length=MAX_TOKENS,\n",
        "        truncation=True,\n",
        "        padding='max_length'\n",
        "    )\n",
        "\n",
        "    # make sure labels are shifted from input ids for prediction task\n",
        "    token_data['labels'] = [input_id[1:] for input_id in token_data[\"input_ids\"]]\n",
        "    token_data['input_ids'] = [token[:-1] for token in token_data['input_ids']]\n",
        "    token_data['attention_mask'] = [mask[:-1] for mask in token_data['attention_mask']]\n",
        "    \n",
        "    \"\"\"\n",
        "    Explanation:\n",
        "    - For every label:\n",
        "        - For every char in the label:\n",
        "            - If the input_ids (label shifted to the right) is EOS, change it to -100.\n",
        "                - This makes sure that only one EOS is left in.\n",
        "    \"\"\"\n",
        "    token_data['labels'] = [\n",
        "        [\n",
        "            token_data['labels'][i][j]\n",
        "            if token_data['input_ids'][i][j] != tokenizer.eos_token_id\n",
        "            else -100 \n",
        "            for j in range(len(token_data['labels'][i]))\n",
        "        ]\n",
        "        for i in range(len(token_data['labels']))\n",
        "    ]\n",
        "    \n",
        "    return token_data\n",
        "\n",
        "# tokenize!\n",
        "ds = ds.map(\n",
        "    tokenize,\n",
        "    batched=True,\n",
        "    remove_columns=['input_text']\n",
        ")\n",
        "\n",
        "# remove any training datums that don't end\n",
        "ds = ds.filter(\n",
        "    lambda x: x['input_ids'].count(tokenizer.eos_token_id)\n",
        ")\n",
        "\n",
        "ds = ds.with_format(type=\"tensorflow\", columns=['input_ids', 'attention_mask', 'labels'])\n",
        "dict_ds = ds.train_test_split(test_size=0.10, shuffle=True, seed=42)\n",
        "tf_ds = dict()\n",
        "for split in ['train', 'test']:\n",
        "    features = {x: dict_ds[split][x] for x in ['input_ids', 'attention_mask']}\n",
        "    tf_ds[split] = tf.data.Dataset.from_tensor_slices((features, dict_ds[split]['labels'])).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLuMdlnei9sw"
      },
      "source": [
        "## Model Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMh5Vhl9Zbnm"
      },
      "source": [
        "# set epochs and learning rate scheduler\n",
        "EPOCHS = 10\n",
        "\n",
        "START_LR = 0.00001\n",
        "MIN_LR = 0.00001\n",
        "MAX_LR = 0.00003 * tpu_strategy.num_replicas_in_sync\n",
        "RAMPUP_EPOCHS = 3\n",
        "SUSTAIN_EPOCHS = 0\n",
        "EXP_DECAY = .8\n",
        "\n",
        "def lrfn(epoch):\n",
        "  if epoch < RAMPUP_EPOCHS:\n",
        "    return (MAX_LR - START_LR)/RAMPUP_EPOCHS * epoch + START_LR\n",
        "  elif epoch < RAMPUP_EPOCHS + SUSTAIN_EPOCHS:\n",
        "    return MAX_LR\n",
        "  else:\n",
        "    return (MAX_LR - MIN_LR) * EXP_DECAY**(epoch-RAMPUP_EPOCHS-SUSTAIN_EPOCHS) + MIN_LR\n",
        "\n",
        "# define callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.LearningRateScheduler(\n",
        "        lrfn,\n",
        "        verbose=True\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0.02,\n",
        "        verbose=1,\n",
        "        patience=1,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "]\n",
        "\n",
        "# make model\n",
        "with tpu_strategy.scope():\n",
        "    model = TFGPT2LMHeadModel.from_pretrained(\n",
        "        'gpt2',\n",
        "        use_cache=False,\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=model.compute_loss\n",
        "    )\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSQyN-lajAAh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba-6ckkCb7l4"
      },
      "source": [
        "# train this boi up\n",
        "hist = model.fit(\n",
        "    tf_ds['train'],\n",
        "    validation_data=tf_ds['test'],\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC9DFcKKcpCm"
      },
      "source": [
        "# define a pipeline so we can try it out\n",
        "definition = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYgiV_NIcqwj"
      },
      "source": [
        "# see what it says for a bunch of words!\n",
        "def generate_words(words):\n",
        "    for word in words:\n",
        "        result = pd.DataFrame(definition(f\"define {word}:\", max_length=150, num_return_sequences=1))\n",
        "        niceprint(f\"<b>{word}</b>\")\n",
        "        niceprint(\"<br>\".join(result['generated_text']))\n",
        "\n",
        "generate_words([\"beans\", \"chetan\", \"koala care\", \"androgyn\", \"black fog\", \"karen\", \"wungus\", \"boppin with the boys\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASbVpTMcr64"
      },
      "source": [
        "# probably push it to the hub so we can keep playing with it\n",
        "model.push_to_hub(\"cactode/gpt2_urbandict_textgen\", use_auth_token='OPENSESAME')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}